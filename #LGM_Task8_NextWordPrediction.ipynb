{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"#LGM_Task8_NextWordPrediction.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPL+TbI9IIdY0gJyNJVPb5k"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"U-Z7aF_rpDE4","executionInfo":{"status":"ok","timestamp":1651598730041,"user_tz":-330,"elapsed":5207,"user":{"displayName":"Harsh Bansal","userId":"14022242986795664987"}}},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import pickle\n","import numpy as np\n","import os\n","import matplotlib.pyplot as plt\n","import pickle\n","import heapq\n","import warnings as wg\n","wg.filterwarnings(\"ignore\")\n","\n","from nltk.tokenize import RegexpTokenizer\n","from keras.models import Sequential, load_model\n","from keras.layers import LSTM\n","from keras.layers.core import Dense, Activation\n","from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","from tensorflow.keras.models import Sequential,load_model\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.callbacks import ReduceLROnPlateau\n","from tensorflow.keras.callbacks import TensorBoard"]},{"cell_type":"code","source":["file = open('1661-0.txt', encoding = 'utf8').read().lower()"],"metadata":{"id":"FxKtlw6NpGFw","executionInfo":{"status":"ok","timestamp":1651598730042,"user_tz":-330,"elapsed":8,"user":{"displayName":"Harsh Bansal","userId":"14022242986795664987"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["print('Corpus lenth:', len(file))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CAiszu_8xO09","executionInfo":{"status":"ok","timestamp":1651598730042,"user_tz":-330,"elapsed":7,"user":{"displayName":"Harsh Bansal","userId":"14022242986795664987"}},"outputId":"b4456b66-1fd0-4374-dc04-27d145d428db"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Corpus lenth: 581888\n"]}]},{"cell_type":"code","source":["file = open('1661-0.txt', 'r', encoding = 'utf8')\n","lines = []\n","\n","for i in file:\n","  lines.append(i)\n"],"metadata":{"id":"KTj_j7_RxU8f","executionInfo":{"status":"ok","timestamp":1651598730042,"user_tz":-330,"elapsed":6,"user":{"displayName":"Harsh Bansal","userId":"14022242986795664987"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["data= \"\"\n","\n","for i in lines:\n","  data = ''.join(lines)\n","\n","data = data.replace('\\n','').replace('\\r','').replace('\\ufeff','')\n","data[:360]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"ew78zM2Xxntx","executionInfo":{"status":"ok","timestamp":1651598735760,"user_tz":-330,"elapsed":5724,"user":{"displayName":"Harsh Bansal","userId":"14022242986795664987"}},"outputId":"e2a828f5-ae9f-4281-bef1-1335944da243"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"Project Gutenberg's The Adventures of Sherlock Holmes, by Arthur Conan DoyleThis eBook is for the use of anyone anywhere at no cost and withalmost no restrictions whatsoever.  You may copy it, give it away orre-use it under the terms of the Project Gutenberg License includedwith this eBook or online at www.gutenberg.netTitle: The Adventures of Sherlock Holme\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["import string\n","\n","translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n","new_data = data.translate(translator)\n","\n","new_data[:500]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"id":"D9LOF4zFyfvg","executionInfo":{"status":"ok","timestamp":1651598735763,"user_tz":-330,"elapsed":33,"user":{"displayName":"Harsh Bansal","userId":"14022242986795664987"}},"outputId":"c0c43614-fe68-451a-b694-626454cf29aa"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Project Gutenberg s The Adventures of Sherlock Holmes  by Arthur Conan DoyleThis eBook is for the use of anyone anywhere at no cost and withalmost no restrictions whatsoever   You may copy it  give it away orre use it under the terms of the Project Gutenberg License includedwith this eBook or online at www gutenberg netTitle  The Adventures of Sherlock HolmesAuthor  Arthur Conan DoyleRelease Date  November 29  2002  EBook  1661 Last Updated  May 20  2019Language  EnglishCharacter set encoding  U'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["z = []\n","\n","for i in data.split():\n","  if i not in z:\n","    z.append(i)\n","\n","data = ' '.join(z)\n","data[:500]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"id":"fvezt6e-2qnT","executionInfo":{"status":"ok","timestamp":1651598743726,"user_tz":-330,"elapsed":7989,"user":{"displayName":"Harsh Bansal","userId":"14022242986795664987"}},"outputId":"1991d326-cc73-44ca-c10b-3c85c88b18e6"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"Project Gutenberg's The Adventures of Sherlock Holmes, by Arthur Conan DoyleThis eBook is for the use anyone anywhere at no cost and withalmost restrictions whatsoever. You may copy it, give it away orre-use under terms Gutenberg License includedwith this or online www.gutenberg.netTitle: HolmesAuthor: DoyleRelease Date: November 29, 2002 [EBook #1661]Last Updated: May 20, 2019Language: EnglishCharacter set encoding: UTF-8*** START OF THIS PROJECT GUTENBERG EBOOK THE ADVENTURES SHERLOCK HOLMES *\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["tokenizer = Tokenizer() \n","tokenizer.fit_on_texts([data])\n","\n","#\n","pickle.dump(tokenizer, open('tokenizer1.pk1', 'wb'))\n","sequence_data = tokenizer.texts_to_sequences([data])[0]\n","sequence_data[:10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CKV8ws_83Qvu","executionInfo":{"status":"ok","timestamp":1651598743736,"user_tz":-330,"elapsed":59,"user":{"displayName":"Harsh Bansal","userId":"14022242986795664987"}},"outputId":"e0b3498f-4fe7-415f-b359-90493f42c829"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[734, 3285, 20, 516, 105, 134, 10, 175, 735, 3286]"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["vocab_size = len(tokenizer.word_index) + 1"],"metadata":{"id":"mqSS6C3G4MxD","executionInfo":{"status":"ok","timestamp":1651598743738,"user_tz":-330,"elapsed":48,"user":{"displayName":"Harsh Bansal","userId":"14022242986795664987"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["sequences = []\n","\n","for i in range(1, len(sequence_data)):\n","    words = sequence_data[i-1:i+1]\n","    sequences.append(words)\n","    \n","print(\"The Length of sequences are: \", len(sequences))\n","sequences = np.array(sequences)\n","sequences[:10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JDSb-Q806ZUs","executionInfo":{"status":"ok","timestamp":1651598743740,"user_tz":-330,"elapsed":50,"user":{"displayName":"Harsh Bansal","userId":"14022242986795664987"}},"outputId":"25d66f95-b97c-49d2-dd8f-40185c1000cf"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["The Length of sequences are:  24990\n"]},{"output_type":"execute_result","data":{"text/plain":["array([[ 734, 3285],\n","       [3285,   20],\n","       [  20,  516],\n","       [ 516,  105],\n","       [ 105,  134],\n","       [ 134,   10],\n","       [  10,  175],\n","       [ 175,  735],\n","       [ 735, 3286],\n","       [3286, 3287]])"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["X= []\n","y= []\n","\n","for i in sequences:\n","  X.append(i[0])\n","  y.append(i[1])\n","\n","X = np.array(X)\n","y = np.array(y)\n","\n","print(\"the data is:\", X[:5])\n","print(\"the responses are:\",y[:5])\n","      "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9QHHgCMSBTxk","executionInfo":{"status":"ok","timestamp":1651598743746,"user_tz":-330,"elapsed":53,"user":{"displayName":"Harsh Bansal","userId":"14022242986795664987"}},"outputId":"9c92d8c3-61b8-462c-99cd-c38bcd9155a8"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["the data is: [ 734 3285   20  516  105]\n","the responses are: [3285   20  516  105  134]\n"]}]},{"cell_type":"code","source":["y = to_categorical(y, num_classes=vocab_size)\n","y[:5]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KQtSR-kWClug","executionInfo":{"status":"ok","timestamp":1651598745500,"user_tz":-330,"elapsed":1804,"user":{"displayName":"Harsh Bansal","userId":"14022242986795664987"}},"outputId":"77f2f680-ce74-4cdd-9d5a-8a7a5104215f"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["model =  Sequential()\n","model.add(Embedding(vocab_size, 10, input_length=1))\n","model.add(LSTM(1000, return_sequences= True))\n","model.add(LSTM(1000))\n","model.add(Dense(1000, activation='relu'))\n","model.add(Dense(vocab_size, activation= 'softmax'))"],"metadata":{"id":"FfjMgcIODkeL","executionInfo":{"status":"ok","timestamp":1651598749136,"user_tz":-330,"elapsed":3642,"user":{"displayName":"Harsh Bansal","userId":"14022242986795664987"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zsJpJvxKG_s5","executionInfo":{"status":"ok","timestamp":1651598751064,"user_tz":-330,"elapsed":1947,"user":{"displayName":"Harsh Bansal","userId":"14022242986795664987"}},"outputId":"f15201de-a546-4b0c-ba50-4de0bcc3c882"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 1, 10)             136000    \n","                                                                 \n"," lstm (LSTM)                 (None, 1, 1000)           4044000   \n","                                                                 \n"," lstm_1 (LSTM)               (None, 1000)              8004000   \n","                                                                 \n"," dense (Dense)               (None, 1000)              1001000   \n","                                                                 \n"," dense_1 (Dense)             (None, 13600)             13613600  \n","                                                                 \n","=================================================================\n","Total params: 26,798,600\n","Trainable params: 26,798,600\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["checkpoint = ModelCheckpoint(\"nextworld1.h5\", monitor='loss', verbose=1,\n","                             save_best_only= True, mode='auto' )\n","reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose=1)\n","\n","logdir = 'logsnextword1'\n","tensorboard_Visualization = TensorBoard(log_dir=logdir)"],"metadata":{"id":"CqRU7iPsHDNo","executionInfo":{"status":"ok","timestamp":1651599981326,"user_tz":-330,"elapsed":6,"user":{"displayName":"Harsh Bansal","userId":"14022242986795664987"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001))"],"metadata":{"id":"Bl1Ifio9IcpT","executionInfo":{"status":"ok","timestamp":1651598751066,"user_tz":-330,"elapsed":16,"user":{"displayName":"Harsh Bansal","userId":"14022242986795664987"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["model.fit(X, y, epochs=40, batch_size=150, callbacks=[checkpoint, reduce, tensorboard_Visualization])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cW8nCn90JbKw","executionInfo":{"status":"ok","timestamp":1651599076927,"user_tz":-330,"elapsed":325876,"user":{"displayName":"Harsh Bansal","userId":"14022242986795664987"}},"outputId":"7323837d-73ea-46ac-f26d-1c243fc4c329"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/40\n","167/167 [==============================] - ETA: 0s - loss: 9.3991\n","Epoch 1: loss improved from inf to 9.39912, saving model to nextworld1.h5\n","167/167 [==============================] - 15s 45ms/step - loss: 9.3991 - lr: 0.0010\n","Epoch 2/40\n","167/167 [==============================] - ETA: 0s - loss: 9.0008\n","Epoch 2: loss improved from 9.39912 to 9.00083, saving model to nextworld1.h5\n","167/167 [==============================] - 7s 44ms/step - loss: 9.0008 - lr: 0.0010\n","Epoch 3/40\n","166/167 [============================>.] - ETA: 0s - loss: 8.7687\n","Epoch 3: loss improved from 9.00083 to 8.76936, saving model to nextworld1.h5\n","167/167 [==============================] - 8s 45ms/step - loss: 8.7694 - lr: 0.0010\n","Epoch 4/40\n","167/167 [==============================] - ETA: 0s - loss: 8.5961\n","Epoch 4: loss improved from 8.76936 to 8.59610, saving model to nextworld1.h5\n","167/167 [==============================] - 7s 45ms/step - loss: 8.5961 - lr: 0.0010\n","Epoch 5/40\n","167/167 [==============================] - ETA: 0s - loss: 8.4637\n","Epoch 5: loss improved from 8.59610 to 8.46371, saving model to nextworld1.h5\n","167/167 [==============================] - 8s 45ms/step - loss: 8.4637 - lr: 0.0010\n","Epoch 6/40\n","167/167 [==============================] - ETA: 0s - loss: 8.3644\n","Epoch 6: loss improved from 8.46371 to 8.36444, saving model to nextworld1.h5\n","167/167 [==============================] - 7s 44ms/step - loss: 8.3644 - lr: 0.0010\n","Epoch 7/40\n","167/167 [==============================] - ETA: 0s - loss: 8.2650\n","Epoch 7: loss improved from 8.36444 to 8.26504, saving model to nextworld1.h5\n","167/167 [==============================] - 7s 44ms/step - loss: 8.2650 - lr: 0.0010\n","Epoch 8/40\n","167/167 [==============================] - ETA: 0s - loss: 8.1493\n","Epoch 8: loss improved from 8.26504 to 8.14934, saving model to nextworld1.h5\n","167/167 [==============================] - 7s 44ms/step - loss: 8.1493 - lr: 0.0010\n","Epoch 9/40\n","167/167 [==============================] - ETA: 0s - loss: 8.0094\n","Epoch 9: loss improved from 8.14934 to 8.00937, saving model to nextworld1.h5\n","167/167 [==============================] - 7s 44ms/step - loss: 8.0094 - lr: 0.0010\n","Epoch 10/40\n","166/167 [============================>.] - ETA: 0s - loss: 7.8271\n","Epoch 10: loss improved from 8.00937 to 7.82693, saving model to nextworld1.h5\n","167/167 [==============================] - 7s 45ms/step - loss: 7.8269 - lr: 0.0010\n","Epoch 11/40\n","167/167 [==============================] - ETA: 0s - loss: 7.6175\n","Epoch 11: loss improved from 7.82693 to 7.61751, saving model to nextworld1.h5\n","167/167 [==============================] - 7s 45ms/step - loss: 7.6175 - lr: 0.0010\n","Epoch 12/40\n","167/167 [==============================] - ETA: 0s - loss: 7.4316\n","Epoch 12: loss improved from 7.61751 to 7.43160, saving model to nextworld1.h5\n","167/167 [==============================] - 7s 45ms/step - loss: 7.4316 - lr: 0.0010\n","Epoch 13/40\n","167/167 [==============================] - ETA: 0s - loss: 7.2574\n","Epoch 13: loss improved from 7.43160 to 7.25738, saving model to nextworld1.h5\n","167/167 [==============================] - 7s 45ms/step - loss: 7.2574 - lr: 0.0010\n","Epoch 14/40\n","167/167 [==============================] - ETA: 0s - loss: 7.0681\n","Epoch 14: loss improved from 7.25738 to 7.06806, saving model to nextworld1.h5\n","167/167 [==============================] - 7s 45ms/step - loss: 7.0681 - lr: 0.0010\n","Epoch 15/40\n","167/167 [==============================] - ETA: 0s - loss: 6.8708\n","Epoch 15: loss improved from 7.06806 to 6.87081, saving model to nextworld1.h5\n","167/167 [==============================] - 7s 45ms/step - loss: 6.8708 - lr: 0.0010\n","Epoch 16/40\n","167/167 [==============================] - ETA: 0s - loss: 6.6639\n","Epoch 16: loss improved from 6.87081 to 6.66389, saving model to nextworld1.h5\n","167/167 [==============================] - 7s 45ms/step - loss: 6.6639 - lr: 0.0010\n","Epoch 17/40\n","167/167 [==============================] - ETA: 0s - loss: 6.4565\n","Epoch 17: loss improved from 6.66389 to 6.45654, saving model to nextworld1.h5\n","167/167 [==============================] - 7s 45ms/step - loss: 6.4565 - lr: 0.0010\n","Epoch 18/40\n","167/167 [==============================] - ETA: 0s - loss: 6.2707\n","Epoch 18: loss improved from 6.45654 to 6.27071, saving model to nextworld1.h5\n","167/167 [==============================] - 8s 45ms/step - loss: 6.2707 - lr: 0.0010\n","Epoch 19/40\n","166/167 [============================>.] - ETA: 0s - loss: 6.0887\n","Epoch 19: loss improved from 6.27071 to 6.08930, saving model to nextworld1.h5\n","167/167 [==============================] - 8s 45ms/step - loss: 6.0893 - lr: 0.0010\n","Epoch 20/40\n","166/167 [============================>.] - ETA: 0s - loss: 5.9176\n","Epoch 20: loss improved from 6.08930 to 5.91817, saving model to nextworld1.h5\n","167/167 [==============================] - 8s 45ms/step - loss: 5.9182 - lr: 0.0010\n","Epoch 21/40\n","166/167 [============================>.] - ETA: 0s - loss: 5.7497\n","Epoch 21: loss improved from 5.91817 to 5.75057, saving model to nextworld1.h5\n","167/167 [==============================] - 8s 45ms/step - loss: 5.7506 - lr: 0.0010\n","Epoch 22/40\n","167/167 [==============================] - ETA: 0s - loss: 5.6075\n","Epoch 22: loss improved from 5.75057 to 5.60749, saving model to nextworld1.h5\n","167/167 [==============================] - 8s 45ms/step - loss: 5.6075 - lr: 0.0010\n","Epoch 23/40\n","167/167 [==============================] - ETA: 0s - loss: 5.4693\n","Epoch 23: loss improved from 5.60749 to 5.46926, saving model to nextworld1.h5\n","167/167 [==============================] - 7s 44ms/step - loss: 5.4693 - lr: 0.0010\n","Epoch 24/40\n","166/167 [============================>.] - ETA: 0s - loss: 5.3337\n","Epoch 24: loss improved from 5.46926 to 5.33466, saving model to nextworld1.h5\n","167/167 [==============================] - 8s 45ms/step - loss: 5.3347 - lr: 0.0010\n","Epoch 25/40\n","167/167 [==============================] - ETA: 0s - loss: 5.2223\n","Epoch 25: loss improved from 5.33466 to 5.22232, saving model to nextworld1.h5\n","167/167 [==============================] - 7s 45ms/step - loss: 5.2223 - lr: 0.0010\n","Epoch 26/40\n","166/167 [============================>.] - ETA: 0s - loss: 5.0988\n","Epoch 26: loss improved from 5.22232 to 5.09960, saving model to nextworld1.h5\n","167/167 [==============================] - 8s 45ms/step - loss: 5.0996 - lr: 0.0010\n","Epoch 27/40\n","167/167 [==============================] - ETA: 0s - loss: 5.0021\n","Epoch 27: loss improved from 5.09960 to 5.00210, saving model to nextworld1.h5\n","167/167 [==============================] - 8s 45ms/step - loss: 5.0021 - lr: 0.0010\n","Epoch 28/40\n","167/167 [==============================] - ETA: 0s - loss: 4.9030\n","Epoch 28: loss improved from 5.00210 to 4.90298, saving model to nextworld1.h5\n","167/167 [==============================] - 7s 45ms/step - loss: 4.9030 - lr: 0.0010\n","Epoch 29/40\n","166/167 [============================>.] - ETA: 0s - loss: 4.8096\n","Epoch 29: loss improved from 4.90298 to 4.80977, saving model to nextworld1.h5\n","167/167 [==============================] - 8s 45ms/step - loss: 4.8098 - lr: 0.0010\n","Epoch 30/40\n","167/167 [==============================] - ETA: 0s - loss: 4.7425\n","Epoch 30: loss improved from 4.80977 to 4.74246, saving model to nextworld1.h5\n","167/167 [==============================] - 8s 45ms/step - loss: 4.7425 - lr: 0.0010\n","Epoch 31/40\n","167/167 [==============================] - ETA: 0s - loss: 4.6582\n","Epoch 31: loss improved from 4.74246 to 4.65821, saving model to nextworld1.h5\n","167/167 [==============================] - 8s 45ms/step - loss: 4.6582 - lr: 0.0010\n","Epoch 32/40\n","167/167 [==============================] - ETA: 0s - loss: 4.5826\n","Epoch 32: loss improved from 4.65821 to 4.58257, saving model to nextworld1.h5\n","167/167 [==============================] - 8s 45ms/step - loss: 4.5826 - lr: 0.0010\n","Epoch 33/40\n","167/167 [==============================] - ETA: 0s - loss: 4.5114\n","Epoch 33: loss improved from 4.58257 to 4.51139, saving model to nextworld1.h5\n","167/167 [==============================] - 8s 45ms/step - loss: 4.5114 - lr: 0.0010\n","Epoch 34/40\n","167/167 [==============================] - ETA: 0s - loss: 4.4433\n","Epoch 34: loss improved from 4.51139 to 4.44331, saving model to nextworld1.h5\n","167/167 [==============================] - 7s 45ms/step - loss: 4.4433 - lr: 0.0010\n","Epoch 35/40\n","167/167 [==============================] - ETA: 0s - loss: 4.3817\n","Epoch 35: loss improved from 4.44331 to 4.38166, saving model to nextworld1.h5\n","167/167 [==============================] - 8s 45ms/step - loss: 4.3817 - lr: 0.0010\n","Epoch 36/40\n","167/167 [==============================] - ETA: 0s - loss: 4.3127\n","Epoch 36: loss improved from 4.38166 to 4.31271, saving model to nextworld1.h5\n","167/167 [==============================] - 8s 46ms/step - loss: 4.3127 - lr: 0.0010\n","Epoch 37/40\n","167/167 [==============================] - ETA: 0s - loss: 4.2515\n","Epoch 37: loss improved from 4.31271 to 4.25153, saving model to nextworld1.h5\n","167/167 [==============================] - 8s 45ms/step - loss: 4.2515 - lr: 0.0010\n","Epoch 38/40\n","167/167 [==============================] - ETA: 0s - loss: 4.1885\n","Epoch 38: loss improved from 4.25153 to 4.18848, saving model to nextworld1.h5\n","167/167 [==============================] - 8s 45ms/step - loss: 4.1885 - lr: 0.0010\n","Epoch 39/40\n","167/167 [==============================] - ETA: 0s - loss: 4.1300\n","Epoch 39: loss improved from 4.18848 to 4.13002, saving model to nextworld1.h5\n","167/167 [==============================] - 8s 45ms/step - loss: 4.1300 - lr: 0.0010\n","Epoch 40/40\n","166/167 [============================>.] - ETA: 0s - loss: 4.0574\n","Epoch 40: loss improved from 4.13002 to 4.05898, saving model to nextworld1.h5\n","167/167 [==============================] - 8s 45ms/step - loss: 4.0590 - lr: 0.0010\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f0c1c286590>"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["model = load_model('nextworld1.h5')\n","tokenizer = pickle.load(open('tokenizer1.pk1','rb'))\n","\n","def predict_next_word(model, tokenizer, text):\n","\n","  sequence = tokenizer.texts_to_sequences([text])\n","  sequence = np.array(sequence)\n","  preds = np.argmax(model.predict(sequence))\n","  predcited_word = \"\"\n","\n","  for key,value in tokenizer.word_index.items():\n","    if value == preds:\n","      predicted_word = key\n","      break\n","\n","  print(predicted_word)\n","  return predcited_word\n"],"metadata":{"id":"_WsAcuCcJ1LG","executionInfo":{"status":"ok","timestamp":1651600022316,"user_tz":-330,"elapsed":3326,"user":{"displayName":"Harsh Bansal","userId":"14022242986795664987"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["while(True):\n","  text= input('enter your line: ')\n","\n","  if text=='0':\n","    print('execution completed......')\n","    break\n","\n","  else:\n","    try:\n","      text = text.split(\" \")\n","      text = text[-3:]\n","      print(text)\n","\n","      predict_next_word(model, tokenizer, text)\n","\n","    except Exception as e:\n","      print('error occured',e)\n","      continue\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mWyZ8f3-B272","executionInfo":{"status":"ok","timestamp":1651600782190,"user_tz":-330,"elapsed":4094,"user":{"displayName":"Harsh Bansal","userId":"14022242986795664987"}},"outputId":"70209092-ae12-438b-9ce1-67170d401cba"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["enter your line: 0\n","execution completed......\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"NiB4iPY4Dl9R"},"execution_count":null,"outputs":[]}]}